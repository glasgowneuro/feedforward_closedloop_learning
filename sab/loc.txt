Reviewer 1
==========


Comment:
This bidirectionnal input/output mapping reminds me of direct/inverse models learning in robotics, which is indeed a very interesting approach but has also some weaknesses, particularly when the mapping is not homogenous in both spaces, where some regions might be underrepresented and yield lower quality of representation in one direction or the other. Do you experience such problems ? More generally, you do not mention situations where your approach might be less pertinent.

Reply:
Indeed. This is a very good point and we have addressed it. We write now:

---------------
Our learning algorithm resembles similarities to work on inverse
models in control generated by neural networks
\cite{Tejomurtula1999}. These models exhibit weaknesses when the
mapping between direct and inverse control is not
homogeneous. However, this weakness arises because backprop is trained
by the error between the direct model and the inverse model because we
need an \textsl{output}. Our model learns based on the error signal at
the \textsl{input} after it has traversed through the environment
which impacts in two ways: learning does not rely on a close
relationship between inverse and direct model while at the same time
it will develop far less rigid or autonomous solutions which are
more compatible with a flexible agent than an engineering system. In other words the more hidden layers we have the
more behavioural flexibility the agent will show.
---------------

Comment:
Nevertheless, whereas I can see inspiration from backprop learning algorithm, I do not really see where the deep approach is important here. In both case you have only two hidden and relatively small layers.

Reply:
We agree that the network is not very deep at the moment but it works with many more layers. The point is that the deeper the network the more flexibility the network can generate. This refers back to the 1st comment. One can tune how rigid it is by adding more or less layers. See the added paragraph from the previous comment which mentions this explicitly and we've also made this clear at the very start of the section "Deep feedback learning":

---------------------------------
\section{Deep feedback learning}
We define a network with an input layer, arbitrary hidden layers and an output
layer which can all have different numbers of neurons (see
Fig~\ref{netw_together}A). In contrast to traditional
--------------------------------


Reviewer 2
==========


Comment:
One criticism of this paper is that in the introduction, a critique of other work is based on very large state-spaces or slow rates of learning, limiting their application to real world scenarios.  This would also seem to apply to this paper, which requires a large number of time-steps to learn, and is currently limited to closed-loop scenarios that don't mimic the real world.  That said, the approach is very interesting, and is one I would be interested in seeing developed further.

Reply:
Thanks for the comment. While the line follower is slow mainly because of a very buggy simulator the shooter game is very fast. One can tweak the network to learn the aiming behaviour within two or three appearances of the enemy which happens at higher learning rates before it becomes unstable and thus approaches one shot learning. We added a sentence to the shooter game results:

----
For high learning rates learning is virtually
instantaneous and approaches one shot learning.
----


Comment:
It is not clear anywhere in the paper whether the layers in the network are full-connected or sparsely connected.  Figure 2A might suggest a sparsely connected network, but this is probably just to maintain the readability of the diagram.  However, if it is sparsely connected, some indication of the amount of connections would be useful.

Reply:
Very good point and also moved the network setup from the caption to the main text. We write now:

-------------------------------------------------------
In order to demonstrate DFL we need a simple closed loop scenario
which can be improved with the help of DFL.
Fig.~\ref{linefollower_robot_playground} shows a simple line following
robot (Fig.~\ref{linefollower_robot_playground}A) which has two wheels
whose speed is controlled by the fixed feedback control and DFL. The
robot has the task of following the line depicted in
Fig.~\ref{linefollower_robot_playground}B to the end, where it
reverses and drives back, and so on. The robot is simulated with an
updated version of ENKI for QT5 (https://github.com/berndporr/enki).
The line follower is using just the ground sensors of the robot to
create the error signal:
\begin{equation}
\mathrm{error} = (g_{l_1}+2 g_{l_2})-(g_{r_1}+2 g_{r_2}) \label{line_error}
\end{equation}
which directly creates a steering reaction from the fixed
feedback loop by controlling the speed of the wheels.
Each of the 30 predictive signals $p_m$ from the two rows of ground
sensors is split into ten 2nd-order lowpass filters with impulse
responses lasting from two timesteps to $30$ timesteps, and then all
feed into the DFL network as predictive inputs $v_j$, giving 300
inputs in total. There were 2 hidden layers with nine and six neurons;
six output neurons. All layers were fully connected. The learning rate
was 0.0001.
-------------------------------------------------------


Comment:
Furthermore, it is not clear until reaching the scenarios what the inputs and outputs might be. Giving a general example e.g. speeds of motors, early on might better help to understand the formulation of the network.

Reply:
We have added an info about the motor outputs earlier in the text. There is already an example for the inputs. This reads now:

-------------------------
The deep feedback learning network receives additional inputs which are able
to predict the disturbance, and thus prevent the trigger of the feedback
loop. These additional inputs are provided via the transfer function $P_1$
and represent the disturbance in a filtered form. For example a video camera
can provide images of the road ahead, or of an enemy. Deep feedback
learning has the task to take the error signal, and tune its network
to generate actions to minimise the error. In the context of the driving
scenario the network output would be a steering action which would help
to improve or replace the rigid feedback loop control.
-------------------------


Comment:
Related to this, why are three neurons used for the outputs of each activity, e.g. wheel speed, and are they really necessary?  In figure 6E, it would appear that the bias in usage of these nodes is also learnt, so learning focuses on a single output with little improvement in the other two.

Reply:
there is an advantage of using more units because the network is non-linear so that the network can "decide" to learn to have decision units. This is quite similar to deep learning where often weights become very high and only limited by the v^2 term in the learning rule. We haven't delved into this more deeply because of space constraints. However we write:

----------------
The output layer of our deep feedback learner consists of 6 neurons
with activations $v_k$ ($k=0 \ldots 5$) - these can be seen as soft
decision-making units where 3 of them determine the change of speed of
the right wheel, and 3 the left wheel.
----------------

Comment:
When presenting the results there is a wide range of different figures given, some of which are more informative than others.  In the case of figure 4, I would suggest focusing of A, D and C.  B is nice to see, possibly alongside G, but the others seem less useful.

Reply:
We do think that this additional information provides more transparency and just removing one of the three layers would throw up suspicion what is happening in that specific layer. Also in this way it is possible for the reader to actually follow the signal from the input to the output.

Comment:
In the paragraph before fig 5, you mention that it is interesting that the initial layer learns faster than the output layer.  This would seem to be intuitive rather than necessarily interesting as the error is being propagated forward, rather than backwards.

Reply:
Indeed this is rather intuitive and there is no need to talk about this at length.

Comment:
Fig 6C, The caption says the graph purely shows the output from the network, but the text on the graph itself would seem to suggest it also includes some output from the reflex controller at the start?  If this is not the case, I'm surprised at the amount of output early on, however the scale is quite small showing only a small amount of change during the course of learning, as compared to the similar sub-figure from Fig 4.

Reply:
The network has been initialised with random weights. Otherwise it cannot learn. We now write:

--------
As before, we see that the controller outputs steadily increase over
time (Fig.~\ref{shooter_results}B). We initialise weights in a uniform distribution, and find that the network produces significant outputs even at the beginning of each learning trial. 
--------

Comment:
Fig 6D, what are the different lines here? This is not really explained anywhere, nor are the lines very easy to distinguish.  Also if I understand this figure correctly, at best the system is being killed 2.5 times more than it is killing the enemy, which seems rather low performance.

Reply:

This are simply plots of KD ratio over time, for 4 example runs. We clarify:

-------------------
Example KD learning curves for 4 different runs - the time series of kills/deaths is filtered with a
2nd order lowpass filter to get a moving average, plotted over time.
-------------------

It is true that the best performance is still considerably below that of the bots from Intel or Facebook - this is expected though, as our bot cannot translate, and only rotates like a fixed turret. This leads to a relatively high number of steps where the bot fails to face the enemy at all, whereas a bot that moves around the environment will naturally make more turns so as to avoid collisions. We write:

---------------------------
The bot's restricted abilities lead to a relatively low KD ratio of 0.4, mainly caused by a high number of steps where it does not see the enemy. 
---------------------------


Comment:
In general, I would say that the figure captions are all too long, and contain details that belong in the body text, rather than in the captions, e.g. parameters for the configuration of the networks in each experiment and the smoothing operation on KD curves.  I would prefer to see the figure captions kept concise.

Reply:
We have moved the parameters to the main text.


Comment:
All sub-figures in figure 4 are missing axis labels, and some in fig 6.  The order in which the figures are presented (e.g. 6F&G) and discussed are also inconsistent.

Reply:
We have added axis labels and fixed the order of the sub-figures. 



Grammar:  This paper is generally very well written, with just a couple of minor issues that can be resolved with a final proof read.  A couple of specific example that I noted:
Section 2, paragraph 1: "input of organism" should probably be "input of the organism"
Figure 5A caption, quote mark wrong way around.
Just below Fig 6, "do statistics" should be "perform statistical analysis"

Comment: Thanks! We have fixed these issues.


To both reviewers:
Thanks for the very detailed and constructive feedback.
